{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ú®This a Jupyter Notebook that allows you to run Python code interactively üêç‚ú®\n",
    "\n",
    "## Getting Started\n",
    "1. Click the **Select Kernel** button on the top right of the notebook.\n",
    "2. Choose **Python environments** and select **Python 3.11.9**.\n",
    "\n",
    "Run code in the notebook by clicking the play button on the left side of the code cells.\n",
    "\n",
    "## Learning Outcomes\n",
    "We will focus on four key outcomes:\n",
    "\n",
    "1. [Understanding agents and prompt engineering with Prompty.](#1-understanding-agents-and-prompt-engineering-with-prompty)\n",
    "2. [Utilizing Prompty tracing for debugging and observability.](#2-utilizing-prompty-tracing-for-debugging-and-observabilty)\n",
    "3. [Building and running Contoso Creative Writer.](#3-building-and-running-contoso-creative-writer)\n",
    "4. [Setting up automated evaluations with GitHub Actions.](#4-setting-up-automated-evaluations-and-deployment-with-github-actions)\n",
    "\n",
    "Let‚Äôs get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Agents and Prompt Engineering with Prompty\n",
    "### 1.1. What are AI agents?\n",
    "Contoso Creative Writer is an Agentic Application. \n",
    "\n",
    "**In artificial intelligence an agent is a program designed to:**\n",
    "\n",
    "- Perceive its environment\n",
    "- Make decisions\n",
    "- Take actions to achieve specific goals\n",
    "\n",
    "For Contoso Creative Writer, the goal is to help the marketing team at Contoso Outdoors write well-researched, product-specific articles. \n",
    "<br>Contoso Creative Writer is made up of 4 agents that help achieve this goal. \n",
    "\n",
    "<img src=\"../../images/agents.png\" alt=\"Agents in Contoso Creative Writer\" width=\"900\" height=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. How is an AI agent built?\n",
    "\n",
    "Each agent in Contoso Creative Writer is built with [Prompty](https://prompty.ai/)! \n",
    "\n",
    "#### 1.2.1 What is Prompty?\n",
    "Prompty is a new asset class and file format for LLM prompts that aims to provide observability, understandability, and portability for developers.\n",
    "\n",
    "**The tooling for Prompty comes together in three ways:**\n",
    "\n",
    "**i. A prompty file:**\n",
    "- A Prompty file is not tied to any language as it uses the markdown format with YAML\n",
    "- The file contains two main parts:\n",
    "    - **Front Mattter:** \n",
    "        - This is the first part of the prompty file \n",
    "        - It is written in YAML and is contained inside two `---` seperators. \n",
    "        - It includes basic details about the prompt, the model configuration and prompty inputs. \n",
    "\n",
    "    - **Prompt Template:** \n",
    "        - This is the base prompt that is sent to the LLM once the prompty is executed. \n",
    "        - It uses Jinja format to pass values either specified in the front matter or from the application to the LLM.\n",
    "        - Given *'name': Marlene*, the variable *{{name}}* will be replaced by *Marlene* at runtime. \n",
    "\n",
    "**ii. The VS Code extension tool:**\n",
    "- The Prompty extension allows you to run Prompty files directly in VS Code. \n",
    "- It has been pre-installed for this workshop, but you can also find it in the Visual Studio Code Marketplace.\n",
    "\n",
    "**iii. Runtimes in multiple programming languages:**\n",
    "To execute a Prompty file asset in code, you can use one of the supporting runtimes such as:\n",
    "- [Prompty Core (python)](https://github.com/microsoft/prompty/blob/main/runtime/prompty/README.md) - The python Prompty core package is the base package needed to run a prompty asset file.\n",
    "- [Promptyflow (python)](https://microsoft.github.io/promptflow/tutorials/prompty-quickstart.html) - Use Prompty with Promptflow as your runtime and logic orchestrator.\n",
    "- [LangChain (python)](https://github.com/langchain-ai/langchain/tree/master/libs/partners/prompty) - Use the experimental LangChain Prompty runtime.\n",
    "- [Semantic Kernal (csharp)](https://www.developerscantina.com/p/semantic-kernel-prompty/) - Use the experimental Semantic Kernal Prompty runtime.\n",
    "\n",
    "### 1.3. Building an AI Agent\n",
    "\n",
    "To help us understand practically how we build an AI agent will build the **Researcher Agent** step by step.\n",
    "<br>In order to build the Researcher agent you will complete the following 3 steps.\n",
    "\n",
    "#### Steps to build the researcher agent\n",
    "- [ ] Step 1: Build a multi-lingual query generator\n",
    "- [ ] Step 2: Give the query generator a list of functions \n",
    "- [ ] Step 3: Build the tools and execute the research\n",
    "\n",
    "##### **Step 1:** Build a multi-lingual query generator\n",
    "- At it's core the researcher agent generates queries that can be used to look for information online. \n",
    "- It also allows us to find search results in multiple languages. \n",
    "- This can all be done using a single prompty file. \n",
    "- To see this in action open the [researcher-0.prompty](researcher/researcher-0.prompty) file and click the run button on the top right of the file. \n",
    "- The generated results from the LLM will be displayed in the *output* tab in the terminal. \n",
    "\n",
    "‚úÖ To complete this step update the *sample* category in the front matter to change the instructions to use a new language.\n",
    "<br> (For example use *es-ES* instead of *en-US*, to get the results back in Spanish) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to executing a Prompty file using the VS Code extension, here's how to execute it using the Prompty Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompty\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "instructions = \"Can you generate queries to find the latest camping trends and what folks are doing in the winter? Use 'en-US' as the market code. \"\n",
    "Markdown(prompty.execute(os.getcwd() + \"/researcher/researcher-0.prompty\", inputs={\"instructions\": instructions}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2:** Give the query generator a list of functions \n",
    "- In order for the researcher to generate even better queries it needs to know which search functions are avaialble to it. \n",
    "- Prompty allows us to pass information to the LLM in the form of a json file using the *${file:filename.json}* format.\n",
    "- In this step we want to give the LLM a list of the functions (sometimes called tools) that it can choose from.\n",
    "- Open the [functions.json](researcher/functions.json) file. Read the descriptions of the **find_information**, **find_entities** and **find_news** functions. \n",
    "- Open the [researcher-1.prompty](researcher/researcher-1.prompty) file and note that *${file:functions.json}* has been added to *tools* under the *parameters* section in the front matter. \n",
    "- Click the run button on the top right of the file and look at the results in the output tab.\n",
    "\n",
    "**Passing a json file to the *tools* parameter in Prompty invokes the LLM to:**\n",
    "- return a list of dictionaries, where each dictionary represents a function to call and its arguments\n",
    "- the *funcion* key contains the generated query and the selected market as arguments\n",
    "- the *name* key contains the name of the most appropriate function to use\n",
    "\n",
    "‚úÖ To complete this step update the instructions passed in the *sample* section of the prompty file to influence which function the LLM chooses. \n",
    "<br>(For example to influence choice of the *find_news* function, use instructions: 'Can you find the latest news about Microsoft?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3:** Understanding how the LLM chooses which function to call\n",
    "\n",
    "**Let's walk through the code**\n",
    "\n",
    "Let's import `execute_researcher_prompty` from the [researcher3.py](./researcher/researcher3.py) script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../docs/workshop/researcher'))\n",
    "\n",
    "from researcher3 import execute_researcher_prompty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Prompty template is configured to load a catalog [functions.json](./researcher/functions.json) with the 3 functions:\n",
    "- `find_information`\n",
    "- `find_entities`\n",
    "- `find_news`\n",
    "\n",
    "Let's test each of one them to get a sense of how the LLM selects them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a.1. Find information\n",
    "\n",
    "Let's see how the LLM can help us find information on the web. It cannot find information per se but it can select amongst a catalog of functions the one which would allow achieving that goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Can you find the best educational material for learning Python programming\"\n",
    "function_calls = execute_researcher_prompty(instructions=instructions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_information` function was selected by the LLM based on the description of the function. It also figured out which parameter values should be passed to the function.\n",
    "\n",
    "Go have a look at the definition of the `find_information` function in [functions.json](./researcher/functions.json) and try to get a sense of why this function was selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a.2. Find entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Who is the inventor of the Python programming language?\"\n",
    "function_calls = execute_researcher_prompty(instructions=instructions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the `find_entities` function called? Why? Go look at the `find_entities` definition in [functions.json](./researcher/functions.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a.3. Find news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"What's the latest about the Python programming language?\"\n",
    "function_calls = execute_researcher_prompty(instructions=instructions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üêû**BUG ALERT:** A bug has purposefully been left in the [functions.json](./researcher/functions.json) file.\n",
    "\n",
    "Which function call has been selected? If any? With which parameters? Is it the `find_information` function we want?\n",
    "\n",
    "**Exercice**:\n",
    "- Find why the `find_news` function is not the one selected by the LLM\n",
    "- Learn how to steer the LLM to select a given function using its description and parameters\n",
    "- Add the function definition for `find_news` to the [functions.json](./researcher/functions.json) file\n",
    "<details>\n",
    "  <summary>find_news function definition</summary>\n",
    "  \n",
    "```json\n",
    "\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"find_news\",\n",
    "      \"description\": \"Finds jokes on the web given a query. This function uses the Bing Search API to find jokes on the web given a query. The response includes the funniest jokes from the web and should be used if you're looking for a laugh.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"An optimal search query to find jokes on the web using the Bing Search API\"\n",
    "          },\n",
    "          \"market\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The market to search in, e.g. en-US - it should match the language of the query\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"query\"\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 4:** Build the functions and execute the research\n",
    "- The researcher has now selected which function to use and has generated a query and market code to pass to it. \n",
    "- We now need to write the Python code for these functions that will pass the queries to the Bing Search API. \n",
    "- We will also use a function in Python to execute the prompty file, instead of running it manually in VS Code. \n",
    "- To put everything together click the play button on the left of the jupyter notebook code cell below below.  \n",
    "- The notebook cell calls the *research* function to run code from **researcher3.py**.\n",
    "- Open the the [researcher3.py](researcher/researcher3.py) file to see this code and try and understand what each function does. \n",
    "\n",
    "‚úÖ To complete this step run the code in the cell below and test it out with different instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from IPython.display import JSON\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../docs/workshop/researcher'))\n",
    "\n",
    "from researcher3 import execute_function_calls\n",
    "\n",
    "instructions = \"Can you find the best educational material for learning Python programming\"\n",
    "\n",
    "# Execute the researcher prompty to get a list of functions calls\n",
    "function_calls = execute_researcher_prompty(instructions=instructions)\n",
    "\n",
    "# Execute the function calls\n",
    "research = execute_function_calls(function_calls)\n",
    "research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congratulations you've build your first AI Agent with Promptyüéâ\n",
    "- [‚úÖ] Step 1: Build a multi-lingual query generator\n",
    "- [‚úÖ] Step 2: Give the query generator a list of functions available\n",
    "- [‚úÖ] Step 3: Build the tools and execute the research\n",
    "\n",
    "We can now succesfully move on to learning outcome 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilizing Prompty tracing for debugging and observabilty\n",
    "\n",
    "When running Applications driven by LLMs, sometimes things don't go as expected! \n",
    "<br>It's important to have a way to debug your LLM workflow so you can see where things are working. \n",
    "<br>Tracing helps you visualize the execution of your prompts and clearly see what inputs are being passed to the LLM. \n",
    "\n",
    "To illustrate how to use local tracing in prompty let's build and debug a custom agent!\n",
    "\n",
    "### 2.1 Run and debug a custom social media agent\n",
    "\n",
    "<img src=\"../../images/socialmediaagent.png\" alt=\"social media agent\" width=\"600\" height=\"350\">\n",
    "\n",
    "You should see a **socialmedia** folder in the workshop folder. This folder contains:\n",
    "\n",
    "- [social.py](socialmedia/social.py) file: Uses the *execute_social_media_writer_prompty* and *run_social_media_agent* functions to send the inputs and prompts to the LLM. \n",
    "<br>It also imports the research function from the researcher agent to let it access information from online.  \n",
    "\n",
    "- [social.prompty](socialmedia/social.prompty) file: This contains the base prompt for the social media agent. \n",
    "<br>It has been instructed to generate a thread of **4 tweets**.\n",
    "\n",
    "üêû**BUG ALERT:** I have purposefully left a bug in the prompty file. We will use tracing to quickly spot the bug and fix it! \n",
    "\n",
    "##### Steps to build and debug the social media agent\n",
    "- [ ] Step 1: Run the code for social media agent\n",
    "- [ ] Step 2: Use Prompty tracing to identify and fix the bug\n",
    "\n",
    "\n",
    "##### **Step 1:** Run the code for social media agent\n",
    "- In order to run the code for social media agent click the play button on the left of the jupyter notebook code cell below. \n",
    "- What do you observe that is strange from the results?  \n",
    "\n",
    "‚úÖ To complete this step run the jupyter notebook cell below and try guess what's causing the bug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../docs/workshop/socialmedia'))\n",
    "\n",
    "from social import run_social_media_agent\n",
    "\n",
    "research_instructions = \"Find information about AI Agents\"\n",
    "social_media_instructions = \"Write a fun and engaging twitter thread about AI Agents given the research.\"\n",
    "\n",
    "run_social_media_agent(instructions=research_instructions, social_media_instructions = social_media_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2:** Use Prompty tracing to identify and fix the bug\n",
    "- To see the trace generated by Prompty open the **workshop** folder in the file explorer and look for a **.runs** folder in it. \n",
    "- Select this folder and click the **execute_social_media_writer_prompty.tracy** file at the top of the folder. \n",
    "- This file shows you information that prompty has sent to or recieved from the LLM. \n",
    "- In this specific case look at the *Completion Tokens* amount. This shows only 200 tokens, which is not enough for us to generate a thread on twitter. \n",
    "- Completion Tokens shows us what the number of tokens allocated for the LLM response, represented by the *max_tokens* parameter in the prompty file.\n",
    "- To fix this bug go to the [social.prompty](socialmedia/social.prompty) file and edit the *max_tokens* amount to make it 1500.\n",
    "\n",
    "‚úÖ To complete this step **rerun the code in the cell above** and confirm the full thread is generated!\n",
    "<br>(Note: If you get an error you may need to restart the notebook by clicking the *restart* button at the top of the notebook and then rerun the cell.)\n",
    "\n",
    "\n",
    "##### Congratulations you've succesfully used Prompty tracing for debuggingüéâ\n",
    "- [‚úÖ] Step 1: Run the code for social media agent\n",
    "- [‚úÖ] Step 2: Use Prompty Tracing to identify and fix the bug\n",
    "\n",
    "Now that we have a good understanding of how to build and debug agents with Prompty let's run Contoso Creative Writer, a multi-agent solution! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and running Contoso Creative Writer \n",
    "\n",
    "#### Steps to build and run Contoso Creative Writer\n",
    "- [ ] Step 1: Start the FastAPI server \n",
    "- [ ] Step 2: Start the Web Server\n",
    "- [ ] Step 3: Test the app\n",
    "\n",
    "**You will need to use the terminal to complete the steps:** \n",
    "- If it‚Äôs not already visible, you can open the terminal by clicking on the hamburger menu at the top left of the page \n",
    "- Click Terminal and select New Terminal.\n",
    "- Copy and paste the commands in each step into the terminal and press enter to run them. \n",
    "\n",
    "##### **Step 1:** Start the FastAPI server \n",
    "\n",
    "1. We'll start by navigating to the correct folder\n",
    "\n",
    "    Run the following command:\n",
    "\n",
    "    ```bash\n",
    "    cd ./src/api\n",
    "    ```\n",
    "\n",
    "2. Next we'll run the FastAPI webserver with the following command \n",
    "\n",
    "    ```bash\n",
    "    fastapi dev main.py\n",
    "    ```\n",
    "\n",
    "3. Next you'll need to change the visibility of the **8000** and **5173** ports to public in the **PORTS** tab. \n",
    "\n",
    "    You can do this by right clicking on the visibility section of the port, selecting port visibility and setting it to public. \n",
    "    <br>The ports tab should look like this:\n",
    "\n",
    "    <img src=\"../../images/ports.png\" alt=\"Screenshot showing setting port-visibility\" width=\"800px\" />\n",
    "\n",
    "‚úÖ To complete this step once the ports are public navigate back to the terminal tab and confirm that you can see *Application startup complete*. \n",
    "\n",
    "##### **Step 2:** Start the web server\n",
    "\n",
    "1. Open a **new terminal** \n",
    "\n",
    "    Once you've completed the above steps. You'll need to open a **new terminal** and navigate to the web folder. \n",
    "    <br>You can open a new terminal by clicking on the hamburger menu at the top left of the page, clicking Terminal and then selecting New Terminal. \n",
    "\n",
    "    In the terminal run the following commands \n",
    "\n",
    "    ```bash\n",
    "    cd ./src/web\n",
    "    ```\n",
    "    \n",
    "2. Next we will install node packages:\n",
    "\n",
    "    ```bash\n",
    "    npm install\n",
    "    ```\n",
    "\n",
    "3. Finally we can run the web app with a local dev web server:\n",
    "    \n",
    "    ```bash\n",
    "    npm run dev\n",
    "    ```\n",
    "\n",
    "4. Navigate to the Application:\n",
    "    - Once you've run the above command you should see an `http://localhost:5173/` link in the terminal. \n",
    "    - Right click the link or click the **open on browser** button that comes up as a Gitub notification in the bottom right corner of the screen. \n",
    "    - If you see a page from Github select the **continue** button. If not skip you should already see your app. \n",
    "    - You should now see the app appear on your screen!\n",
    "\n",
    "‚úÖ To complete this step confirm that you can see the Constoso Creative Writer application. \n",
    "\n",
    "##### **Step 3:** Test the app\n",
    "\n",
    "- You can now test out the app by clicking **Example** to fill out the example information. Read the example instructions. \n",
    "- Click **Start Work** button to get Contoso Creative Writer to generate an article. \n",
    "- Click on the small **Debug** button at the bottom right of the Application to see which agent steps are carried out.\n",
    "\n",
    "‚úÖ To complete this step confirm that an article was generated that includes citation links and the products requested. \n",
    "\n",
    "##### Congratulations you've succesfully built and ran Contoso Creative Writerüéâ\n",
    "- [‚úÖ] Step 1: Start the FastAPI server \n",
    "- [‚úÖ] Step 2: Start the Web Server\n",
    "- [‚úÖ] Step 3: Test the app\n",
    "\n",
    "You've now almost completed the workshop, let's move on to the final learning outcome!\n",
    "\n",
    "‚è∞**Note:** The next step takes a bit longer to complete: \n",
    "- Remember this lab is self paced so you can always complete it on your own, even after the session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up automated evaluations and deployment with Github Actions \n",
    "\n",
    "Contoso Creative Writer is set up to run a CI/CD pipeline, which stands for Continuous Integration and Continuous Deployment. \n",
    "\n",
    "In this sample code the CI/CD pipeline includes the following: \n",
    "1. **Build and Deploy:** Automatically building and deploying the latest version of the code in production (This helps us confirm things are working as expected.)\n",
    "2. **Evaulations:** Automatically running evaulations on a test article to see how fluent, grounded, relevant and coherent the final response was.\n",
    "\n",
    "#### Steps to run automated evaluations and deployment \n",
    "- [ ] Step 1: Run azd up to deploy your application to production\n",
    "- [ ] Step 2: Set up the azd pipeline configuration\n",
    "- [ ] Step 3: Allow workflows in Github\n",
    "- [ ] Step 4: Examine evaluations \n",
    "\n",
    "##### **Step 1:** Run azd up to deploy your application to production\n",
    "\n",
    "In order to set up a CI/CD pipleline for this project we need to deploy our application to production. \n",
    "\n",
    "- Run the following command in the terminal to deploy the application:\n",
    "\n",
    "    ```bash\n",
    "    azd up\n",
    "    ```\n",
    "- This step will likely take several minutes. So feel free to take a break by stretching and walking around!\n",
    "- Once complete scroll up in your terminal beyond any output from Github and look for *--- üéâ | 4. Access your ACA deployed web app here:*\n",
    "- This is a link to your deployed application. \n",
    "- While the subscription is active you can share this link with friends to show them what you've built! \n",
    "\n",
    "##### **Step 2:** Set up the azd pipeline configuration\n",
    "We will set up the CI/CD pipeline with azd and GitHub actions. To do this **open a new terminal**. \n",
    "\n",
    "- Run the following command:\n",
    "\n",
    "    ```shell\n",
    "    azd pipeline config\n",
    "    ```\n",
    "- select an environment name like yourname-aitour\n",
    "- select the recommended subscription by pressing enter\n",
    "- select `Canada East` as the Azure Location \n",
    "- When asked to Log in using the Github CLI type in `Y`\n",
    "- Choose `HTTPS` as the preferred protocol for Git Operations \n",
    "- Select `Y` to Authenticate with your Github credentials. \n",
    "- Choose Login with a web browser to authenticate Github CLI and follow the authentication instructions. \n",
    "- You may be asked if you want to commit and push your local changes. Choose `Y`\n",
    "- You should see two links in your terminal. Select the sencond link to view your pipeline status. \n",
    "\n",
    "##### **Step 3:** Allow workflows in Github\n",
    "- You will now see a screen in Github that tells you workflows are not being run on the fork. \n",
    "- Click the green button that says 'I understand my workflows go ahead and enable them.'\n",
    "- If the screen is blank come back to the terminal and rerun the *azd pipeline config* command. \n",
    "- Again, select the sencond link to view your pipeline status. \n",
    "- You should now see the two Github action workflows running on the screen. \n",
    "- Look at the subheading of the actions and wait till the *Evaluate* one turns green. \n",
    "- It will likely take a few minutes to complete but once complete click the Evaulate action. \n",
    "\n",
    "##### **Step 4:** Examine evaluations\n",
    "\n",
    "- You should see a table with some scores for relevance, fluencey, coherence and groundedness. \n",
    "- The scores are from 1-5, with 5 being the highest mark. These are used to help us know how well the model is performing.\n",
    "- Examine the evaluations and think of some ways you might be able to improve a score in the future. \n",
    "\n",
    "##### Congratulations you've succesfully automated evaluations and deploymentüéâ\n",
    "- [‚úÖ] Step 1: Run azd up to deploy your application to production\n",
    "- [‚úÖ] Step 2: Set up the azd pipeline configuration\n",
    "- [‚úÖ] Step 3: Allow workflows in Github\n",
    "- [‚úÖ] Step 4: Examine evaluations \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You have now completed the workshop! \n",
    "\n",
    "Today you learnt to:\n",
    "1. Understand agents and prompt engineering with Prompty\n",
    "2. Utilize Prompty tracing for debugging and observability\n",
    "3. Build and run Contoso Creative Writer\n",
    "4. Set up automated evaluations with GitHub Actions\n",
    "\n",
    "We hope you enjoyed this workshop! You can continue to explore the code at [https://aka.ms/aitour/wrk551](https://aka.ms/aitour/wrk551).\n",
    "<br>‚≠êPlease star the [Github repo](https://aka.ms/aitour/wrk551) if you enjoyed this workshop‚≠ê"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
