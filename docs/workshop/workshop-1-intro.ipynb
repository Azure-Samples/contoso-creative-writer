{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ú®This is a Jupyter Notebook that allows you to run Python code interactively üêç‚ú®\n",
    "\n",
    "üö®**Double click on the notebook tab** above to keep the notebook open! \n",
    "\n",
    "**If you have not already:**\n",
    "1. Click the **Select Kernel** button on the top right of the notebook.\n",
    "2. Choose **Python environments** and select **Python 3.11**.\n",
    "\n",
    "Note: To run the code in the cells of this notebook click the play button (‚ñ∂Ô∏è) on the left side of the cells that contain code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Agents and Prompt Engineering with Prompty\n",
    "## 1.1. What are AI agents?\n",
    "Contoso Creative Writer is an Agentic Application. \n",
    "\n",
    "**In artificial intelligence an agent is a program designed to:**\n",
    "\n",
    "- Perceive its environment\n",
    "- Make decisions\n",
    "- Take actions to achieve specific goals\n",
    "\n",
    "For Contoso Creative Writer, the goal is to help the marketing team at Contoso Outdoors write well-researched, product-specific articles. \n",
    "\n",
    "Contoso Creative Writer is made up of 4 agents that help achieve this goal. \n",
    "\n",
    "<img src=\"../../images/agents.png\" alt=\"Agents in Contoso Creative Writer\" width=\"900\" height=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. How is an AI agent built?\n",
    "\n",
    "Each agent in Contoso Creative Writer is built with [Prompty](https://prompty.ai/)! \n",
    "\n",
    "### 1.2.1 What is Prompty?\n",
    "Prompty is a new asset class and file format for LLM prompts that aims to provide observability, understandability, and portability for developers.\n",
    "\n",
    "**The Prompty file:**\n",
    "- A Prompty file is not tied to any language as it uses the markdown format with YAML\n",
    "- The file contains two main parts:\n",
    "\n",
    "    - **Front Mattter:** \n",
    "        - This is the first part of the prompty file \n",
    "        - It is written in YAML and is contained inside two `---` seperators. \n",
    "        - It includes basic details about the prompt, the model configuration and prompty inputs. \n",
    "\n",
    "\n",
    "            <details>\n",
    "            <summary>üí°Click to see an example</summary>\n",
    "            \n",
    "            ```yaml\n",
    "            ---\n",
    "            name: My Prompty File \n",
    "            description: >-\n",
    "            This is a prompt about Prompty\n",
    "            authors:\n",
    "            - Seth Juarez\n",
    "            model:\n",
    "            api: chat\n",
    "            configuration: \n",
    "                type: azure_openai\n",
    "                azure_deployment: gpt-35-turbo\n",
    "                api_version: 2023-07-01-preview\n",
    "            sample:\n",
    "            instructions: Can you tell me more about Prompty?  \n",
    "            ---\n",
    "            ```\n",
    "            </details><br>\n",
    "\n",
    "    - **Prompt Template:** \n",
    "        - This is the base prompt that is sent to the LLM once the prompty is executed. \n",
    "        - It uses Jinja format to pass values either specified in the front matter or from the application to the LLM.\n",
    "        - Given *'name': Marlene*, the variable *{{name}}* will be replaced by *Marlene* at runtime. \n",
    "\n",
    "\n",
    "            <details>\n",
    "            <summary>üí°Click to see an example</summary>\n",
    "            \n",
    "            ```yaml\n",
    "            \n",
    "            system:\n",
    "            You are a helfpul assistant that uses gpt-35-turbo to answer questions about Prompty. \n",
    "            You provide helpful information and reply in a friendly tone\n",
    "\n",
    "            user:\n",
    "            {{instructions}}\n",
    "            ```\n",
    "            </details><br>\n",
    "\n",
    "**The VS Code extension tool:**\n",
    "- The Prompty extension allows you to run Prompty files directly in VS Code. \n",
    "- It has been pre-installed for this workshop, but you can also find it in the Visual Studio Code Marketplace.\n",
    "\n",
    "We'll look at how to use both of these to build and run an AI Agent next. \n",
    "\n",
    "## 1.3. Building an AI Agent\n",
    "\n",
    "To help us understand practically how we build an AI agent, we will build the **Researcher Agent** step by step.\n",
    "\n",
    "<div class='alert alert-block alert-success'>\n",
    "<p>In order to build the Researcher agent you will complete the following 3 steps:</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "> - **Step 1:** Build a multi-lingual query generator\n",
    "> - **Step 2:** Understanding LLM function calling with Prompty\n",
    "> - **Step 3:** Build the tools and execute the research\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's start with step 1. \n",
    "\n",
    "### **Step 1: Build a multi-lingual query generator**\n",
    "The researcher agent generates queries that can be used to look for information online. \n",
    "<br>It also allows us to find search results in multiple languages. \n",
    "\n",
    "Complete the following tasks...\n",
    "\n",
    "---\n",
    "<img src=\"todo.png\" alt=\"todo icon\" width=\"40\" height=\"40\"> **Tasks for you to do:**\n",
    "\n",
    "> **TODO 1:** Open the [researcher-0.prompty](researcher/researcher-0.prompty) file, read the prompt and **click the play button** on the top right of the file.\n",
    "> - You will be prompted to sign into an account. **Choose your skillable email/username to sign in.**\n",
    ">\n",
    "> - Note: If you signed in with the wrong account by mistake sign out of that account in the profile section at the bottom right of the codespace and rerun the Prompty file.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Observations üëÄ:**\n",
    ">   - Observe the output in the terminal. \n",
    ">   - Look in the Prompty file, notice the following instructions in the **sample section**:\n",
    ">   <br>**instructions:** Can you generate queries to find the latest camping trends and what folks are doing in the winter? Use 'en-US' as the market code.*\n",
    "\n",
    "<br> \n",
    "\n",
    "> **TODO 2:** Edit the instructions to use a new language. (For example use **es-ES instead of en-US**, to get the results back in Spanish)\n",
    "\n",
    "**Step 1 Complete ‚úÖ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Understanding LLM function calling with Prompty**\n",
    "In order for the researcher to generate even better queries it needs to know which search functions are avaialble to it. \n",
    "- Using the Prompty **tools** parameter an LLM can choose from functions described in a json file. \n",
    "- We can add information about which functions (sometimes called tools), the LLM has access to in a **functions.json** file. \n",
    "- Information from a json file is passed to prompty using the  *${file:functions.json}* format. \n",
    "\n",
    "In the case of the researcher we have a **functions.json** file with descriptions of 3 functions:\n",
    "- find_information\n",
    "- find_entities\n",
    "- find_news\n",
    "\n",
    "Open [functions.json](./researcher/functions.json) and read the description of the **find_information** function. \n",
    "\n",
    "\n",
    "Complete the following tasks...\n",
    "\n",
    "Its important to note that **Prompty files can also be executed using Python code.** \n",
    "<br>To see how Prompty works with tools we will switch to using code instead of pressing play on the file. \n",
    "\n",
    "---\n",
    "<img src=\"coding.png\" alt=\"todo icon\" width=\"45\" height=\"45\"> **Run the code:**\n",
    "\n",
    "> **TODO 1:** Execute the [researcher-2.prompty](researcher/researcher-2.prompty) file by running the cell below:\n",
    "\n",
    "(Note if you get an error running this cell click the **Restart** button at the top of the notebook and try again.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompty\n",
    "import prompty.azure\n",
    "import os\n",
    "\n",
    "instructions = \"Can you find the best educational material for learning Python programming?\"\n",
    "prompty.execute(os.getcwd() + \"/researcher/researcher-2.prompty\", inputs={\"instructions\": instructions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations üëÄ:**\n",
    ">   - Observe the results. What do you see is different from the previous output?\n",
    ">   - Navigate to [researcher-2.prompty](researcher/researcher-2.prompty) and notice that *${file:functions.json}* has been added to **tools** under the *parameters* section in the  file.\n",
    ">   - What is the name of the tool called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result from running the Prompty file we saw that the **find_information** function was selected. \n",
    "- From its description in the [functions.json](./researcher/functions.json) file can you understand why it was chosen? \n",
    "- Observe that it finds general information on the web. \n",
    "- The LLM used the **instruction** we gave it and the **descriptions** it saw in *functions.json* to pick which function to call.  \n",
    "- It also figured out which parameter values should be passed to the function. \n",
    "\n",
    "We can influence which function is called by being more specific about the instructions we give the LLM. This is a form of **Prompt Engineering**.\n",
    "\n",
    "Complete the following tasks...\n",
    "\n",
    "---\n",
    "<img src=\"coding.png\" alt=\"todo icon\" width=\"45\" height=\"45\"> **Run the code:**\n",
    "\n",
    "> **TODO 2:** Get the LLM to use the **find_entities** function to help us find people, places or things by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Who is the person who invented the Python programming language?\"\n",
    "prompty.execute(os.getcwd() + \"/researcher/researcher-2.prompty\", inputs={\"instructions\": instructions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations üëÄ:**\n",
    ">\n",
    "> Note that the  **find_entities** function was selected by the LLM based on:\n",
    "> 1. **The description of the function** in [functions.json](./researcher/functions.json) \n",
    "> 2. The **instructions** we passed to it. \n",
    "\n",
    "<br>\n",
    "\n",
    "> **TODO 3:** Let's try to get the LLM to call the **find_news** function by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Find the latest news about Microsoft?\"\n",
    "prompty.execute(os.getcwd() + \"/researcher/researcher-2.prompty\", inputs={\"instructions\": instructions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "<p>üêûBUG ALERT: A bug has purposefully been left in the functions.json file.</p>\n",
    "</div>\n",
    "\n",
    "> Observations üëÄ:\n",
    ">\n",
    ">- Which function call has been selected, if any?\n",
    ">- Is this the function we want?\n",
    ">- The find_news function has not been selected by the LLM.\n",
    ">- Look in the functions.json file and see what's wrong.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **TODO 4:** Add the function description for find_news to the functions.json file. (üí°Click the play icon for the details.)\n",
    "Once added rerun the cell above!\n",
    "\n",
    "**find_news function description:**\n",
    "<details>\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"find_news\",\n",
    "      \"description\": \"Finds news on the web given a query. This function uses the Bing Search API to find news on the web given a query. The response includes the most relevant news articles from the web and should be used if you're looking for news.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"An optimal search query to find news on the web using the Bing Search API\"\n",
    "          },\n",
    "          \"market\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The market to search in, e.g. en-US - it should match the language of the query\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"query\"\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  ```\n",
    "  <details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 Complete ‚úÖ**\n",
    "\n",
    "### **Step 3: Build the functions and execute the research**\n",
    "We saw that the researcher:\n",
    "- Selects which function to call\n",
    "- Generates a query to pass to the function\n",
    "- Selects a market code to pass to the function. \n",
    "\n",
    "When we execute a Prompty file that has a **functions.json** file added to the **tools** parameter, the LLM returns a **list of Tool Calls** (also known as function calls) that look like this:\n",
    "\n",
    "```python\n",
    "[ToolCall(id='call_JtomZ3gCGHEa5MBxy6M3vypv', name='find_entities', arguments='{\"query\":\"inventor of Python programming language\",\"market\":\"en-US\"}')]\n",
    "```\n",
    "\n",
    "But where are the functions it should be calling?\n",
    "- The Python code for the functions described in *functions.json* can be found in the [researcher3.py](researcher/researcher3.py) file. \n",
    "- These functions will pass the query and market code to the Bing Search API. \n",
    "- Open the [researcher3.py](researcher/researcher3.py) file and try and find the **find_information, find_news, find_entities** functions. \n",
    "\n",
    "To put everything together the **research** function in [researcher3.py](researcher/researcher3.py) calls:\n",
    "-  an **execute_researcher_prompty** function that has the code we saw earlier to execute the prompty file \n",
    "- an **execute_function_calls** function that runs code to execute all the functions calls\n",
    "\n",
    "Complete the final task...\n",
    "\n",
    "---\n",
    "<img src=\"coding.png\" alt=\"todo icon\" width=\"45\" height=\"45\"> **Run the code:**\n",
    "\n",
    "> **TODO:** Run the cell below to run the full researcher agent! \n",
    "> - Try different instructions and see which results you get and which function is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../docs/workshop/researcher'))\n",
    "\n",
    "from researcher3 import execute_researcher_prompty, execute_function_calls\n",
    "\n",
    "instructions = \"Can you find the best educational material for learning Python programming\"\n",
    "\n",
    "# Execute the researcher prompty to get a list of functions calls\n",
    "function_calls = execute_researcher_prompty(instructions=instructions)\n",
    "\n",
    "# Execute the function calls\n",
    "research = execute_function_calls(function_calls)\n",
    "research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 Complete ‚úÖ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congratulations you've succesfully built your first AI Agent with Promptyüéâ\n",
    "- [‚úÖ] Step 1: Build a multi-lingual query generator\n",
    "- [‚úÖ] Step 2: Understanding LLM function calling with Prompty\n",
    "- [‚úÖ] Step 3: Build the tools and execute the research\n",
    "\n",
    "We can now succesfully move on to learning outcome 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è **Open [next workshop notebook](./workshop-2-tracing.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
